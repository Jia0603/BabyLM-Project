model_name: "LMSeed/GPT2-small-distilled-900M" # Student model
log_with: wandb
learning_rate: 1.0e-6

hf_org : LMSeed # Save the model in my account
save_base_dir: models/

batch_size: 4         # Number of prompts per batch
num_generations: 4    # Group size (G). Total batch size = batch_size * G
mini_batch_size: 2    # For gradient accumulation if G is large
gradient_accumulation_steps: 2
beta: 0.04            # KL penalty coefficient
token_limit: 1000000
checkpoint_interval: 200000

#query settings (range for sampling length of student input)
query_min_length: 1
query_max_length: 8

